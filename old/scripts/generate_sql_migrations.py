#!/usr/bin/env python3
"""
Generate PostgreSQL migration files from YAML stream configurations.

This script reads all _stream.yaml files and generates CREATE TABLE statements
with proper columns, indexes, and constraints based on the schema definitions.

Usage:
    python scripts/generate_sql_migrations.py [--output-dir core/migrations]
"""

import sys
import yaml
from pathlib import Path
from typing import Dict, Any, List
from datetime import datetime
import argparse

TYPE_MAPPING = {
    'string': 'TEXT',
    'text': 'TEXT',
    'integer': 'INTEGER',
    'bigint': 'BIGINT',
    'float': 'DOUBLE PRECISION',
    'boolean': 'BOOLEAN',
    'timestamp': 'TIMESTAMPTZ',
    'jsonb': 'JSONB',
}


def load_yaml(filepath: Path) -> Dict[str, Any]:
    """Load a YAML file safely."""
    if not filepath.exists():
        return {}

    with open(filepath, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f) or {}


def generate_column_definition(col: Dict[str, Any]) -> str:
    """Generate SQL column definition from YAML column spec."""
    name = col['name']
    col_type = col['type']

    # Map to PostgreSQL type
    pg_type = TYPE_MAPPING.get(col_type, 'TEXT')

    # Add max_length for TEXT fields
    if col_type == 'string' and 'max_length' in col:
        pg_type = f"VARCHAR({col['max_length']})"

    # Nullable constraint
    nullable = col.get('nullable', True)
    null_constraint = 'NOT NULL' if not nullable else ''

    # Default value
    default = col.get('default')
    default_clause = ''
    if default is not None:
        if isinstance(default, bool):
            default_clause = f"DEFAULT {str(default).upper()}"
        elif isinstance(default, (int, float)):
            default_clause = f"DEFAULT {default}"
        else:
            default_clause = f"DEFAULT '{default}'"

    parts = [name, pg_type, null_constraint, default_clause]
    return ' '.join(filter(None, parts))


def generate_table_sql(stream_name: str, schema: Dict[str, Any]) -> str:
    """Generate CREATE TABLE SQL for a stream."""
    table_name = schema.get('table_name', f'stream_{stream_name}')
    description = schema.get('description', '')
    columns = schema.get('columns', [])
    indexes = schema.get('indexes', [])

    # Start table definition
    sql = f"-- {description}\n"
    sql += f"CREATE TABLE IF NOT EXISTS {table_name} (\n"
    sql += "    id BIGSERIAL PRIMARY KEY,\n"

    # Add source_id for linking to sources table
    sql += "    source_id TEXT NOT NULL REFERENCES sources(id) ON DELETE CASCADE,\n"

    # Add standard timestamp column (required for all streams)
    sql += "    timestamp TIMESTAMPTZ NOT NULL,\n"

    # Add custom columns
    for col in columns:
        col_def = generate_column_definition(col)
        sql += f"    {col_def},\n"

    # Add audit columns
    sql += "    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n"
    sql += "    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()\n"
    sql += ");\n\n"

    # Add indexes
    for idx in indexes:
        cols = idx.get('columns', [])
        idx_type = idx.get('type', 'btree')
        idx_desc = idx.get('description', '')
        unique = idx.get('unique', False)

        if not cols:
            continue

        col_list = ', '.join(cols)
        idx_name = f"idx_{table_name.replace('stream_', '')}_{cols[0].replace('_', '')}"
        unique_clause = 'UNIQUE ' if unique else ''

        sql += f"-- {idx_desc}\n" if idx_desc else ""
        sql += f"CREATE {unique_clause}INDEX IF NOT EXISTS {idx_name} ON {table_name}({col_list}) USING {idx_type};\n"

    # Add comment on table
    if description:
        sql += f"\nCOMMENT ON TABLE {table_name} IS '{description}';\n"

    return sql + "\n"


def generate_migration_file(sources_dir: Path, output_file: Path):
    """Generate complete migration file from all stream configurations."""

    sql_content = f"""-- Auto-generated stream table migrations
-- Generated at: {datetime.utcnow().isoformat()}Z
-- Source: YAML stream configurations in sources/
--
-- DO NOT EDIT THIS FILE MANUALLY
-- To update schemas, modify the _stream.yaml files and regenerate
--
-- Naming convention: stream_{{source}}_{{stream}}
-- Examples: stream_ios_location, stream_google_calendar, stream_mac_apps

"""

    stream_count = 0

    # Walk through each source directory
    for source_dir in sorted(sources_dir.iterdir()):
        if not source_dir.is_dir() or source_dir.name.startswith(('_', '.', 'base', 'validation')):
            continue

        source_name = source_dir.name

        # Process each stream directory
        for stream_dir in sorted(source_dir.iterdir()):
            if not stream_dir.is_dir() or stream_dir.name.startswith(('_', '.')):
                continue

            # Load stream configuration
            stream_yaml_path = stream_dir / '_stream.yaml'
            if stream_yaml_path.exists():
                stream_config = load_yaml(stream_yaml_path)

                if stream_config and 'schema' in stream_config:
                    stream_name = f"{source_name}_{stream_dir.name}"
                    sql_content += f"-- {source_name.upper()} - {stream_dir.name}\n"
                    sql_content += generate_table_sql(
                        stream_name, stream_config['schema'])
                    sql_content += "\n" + "-" * 80 + "\n\n"
                    stream_count += 1
                    print(f"  ‚úì Generated schema for: {stream_name}")

    # Write to file
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(sql_content)

    print(f"\n‚úÖ Generated migration file: {output_file}")
    print(f"   Total streams: {stream_count}")


def main():
    parser = argparse.ArgumentParser(
        description='Generate SQL migrations from YAML stream configs')
    parser.add_argument('--output-dir', default='core/migrations',
                        help='Output directory for migration file')
    parser.add_argument(
        '--output-name', default='005_auto_generated_streams.sql', help='Output filename')
    args = parser.parse_args()

    # Determine sources directory
    repo_root = Path(__file__).parent.parent
    sources_dir = repo_root / 'sources'

    if not sources_dir.exists():
        print(f"‚ùå Error: sources directory not found at {sources_dir}")
        sys.exit(1)

    # Determine output path
    output_dir = repo_root / args.output_dir
    output_dir.mkdir(parents=True, exist_ok=True)
    output_file = output_dir / args.output_name

    print(f"üîß Generating SQL migrations from YAML configurations...")
    print(f"   Sources: {sources_dir}")
    print(f"   Output: {output_file}\n")

    generate_migration_file(sources_dir, output_file)


if __name__ == '__main__':
    main()
