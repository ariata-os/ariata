{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP Visualization of Benjamin Franklin's Autobiography\n",
    "\n",
    "This notebook creates a UMAP visualization of events extracted from Benjamin Franklin's autobiography using:\n",
    "1. W5H (Who, What, When, Where, Why, How) event extraction\n",
    "2. Sentence transformers for embeddings\n",
    "3. UMAP for dimensionality reduction\n",
    "4. Interactive visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Packages should already be installed. If not, run in terminal:\n# uv pip install requests beautifulsoup4 spacy sentence-transformers umap-learn plotly pandas numpy tqdm scikit-learn\n# uv pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl\n\nprint(\"Packages should be installed. If you see import errors, run the commands above in terminal.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import requests\nimport re\nimport pandas as pd\nimport numpy as np\nfrom typing import List, Dict, Tuple, Optional\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# NLP libraries\nimport spacy\nfrom sentence_transformers import SentenceTransformer\n\n# Visualization\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Dimensionality reduction\nimport umap\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import DBSCAN\n\n# Progress tracking\nfrom tqdm import tqdm\ntqdm.pandas()\n\n# Load spaCy model - use the installed package\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\nnlp.max_length = 2000000  # Increase max length for processing"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def download_franklin_autobiography() -> str:\n    \"\"\"Download Benjamin Franklin's autobiography from Project Gutenberg.\"\"\"\n    # Updated URL - using the plain text version\n    url = \"https://www.gutenberg.org/files/148/148-0.txt\"\n    \n    print(\"Downloading Benjamin Franklin's autobiography...\")\n    response = requests.get(url)\n    \n    if response.status_code == 200:\n        text = response.text\n        print(f\"Downloaded {len(text)} characters\")\n        return text\n    else:\n        raise Exception(f\"Failed to download: {response.status_code}\")\n\n# Download the text\nraw_text = download_franklin_autobiography()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_gutenberg_text(text: str) -> str:\n",
    "    \"\"\"Clean Project Gutenberg text by removing headers, footers, and extra whitespace.\"\"\"\n",
    "    \n",
    "    # Find the actual content boundaries\n",
    "    start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK\"\n",
    "    end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
    "    \n",
    "    start_idx = text.find(start_marker)\n",
    "    if start_idx != -1:\n",
    "        start_idx = text.find('\\n', start_idx) + 1\n",
    "        text = text[start_idx:]\n",
    "    \n",
    "    end_idx = text.find(end_marker)\n",
    "    if end_idx != -1:\n",
    "        text = text[:end_idx]\n",
    "    \n",
    "    # Clean up formatting\n",
    "    text = re.sub(r'\\[Illustration[^\\]]*\\]', '', text)  # Remove illustration tags\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)  # Remove footnote references\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)  # Normalize paragraph breaks\n",
    "    text = re.sub(r' {2,}', ' ', text)  # Remove extra spaces\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Clean the text\n",
    "cleaned_text = clean_gutenberg_text(raw_text)\n",
    "print(f\"Cleaned text: {len(cleaned_text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(text: str, chunk_size: int = 1000) -> List[str]:\n",
    "    \"\"\"Split text into chunks by paragraphs, keeping chunks under specified size.\"\"\"\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        para_size = len(paragraph.split())\n",
    "        \n",
    "        if current_size + para_size > chunk_size and current_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [paragraph]\n",
    "            current_size = para_size\n",
    "        else:\n",
    "            current_chunk.append(paragraph)\n",
    "            current_size += para_size\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Split into manageable chunks\n",
    "text_chunks = split_into_chunks(cleaned_text, chunk_size=500)\n",
    "print(f\"Split into {len(text_chunks)} chunks\")\n",
    "print(f\"Average chunk size: {np.mean([len(c.split()) for c in text_chunks]):.0f} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. W5H Event Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class W5HEvent:\n",
    "    \"\"\"Represents an event extracted from text with W5H components.\"\"\"\n",
    "    who: List[str] = field(default_factory=list)\n",
    "    what: str = \"\"\n",
    "    when: List[str] = field(default_factory=list)\n",
    "    where: List[str] = field(default_factory=list)\n",
    "    why: str = \"\"\n",
    "    how: str = \"\"\n",
    "    original_text: str = \"\"\n",
    "    chunk_index: int = 0\n",
    "    \n",
    "    def to_sentence(self) -> str:\n",
    "        \"\"\"Convert W5H event to a natural language sentence.\"\"\"\n",
    "        parts = []\n",
    "        \n",
    "        # Who\n",
    "        if self.who:\n",
    "            who_str = ', '.join(self.who[:2])  # Limit to 2 main entities\n",
    "            parts.append(who_str)\n",
    "        else:\n",
    "            parts.append(\"Someone\")\n",
    "        \n",
    "        # What\n",
    "        if self.what:\n",
    "            parts.append(self.what)\n",
    "        else:\n",
    "            parts.append(\"did something\")\n",
    "        \n",
    "        # Where\n",
    "        if self.where:\n",
    "            parts.append(f\"in {', '.join(self.where[:1])}\")\n",
    "        \n",
    "        # When\n",
    "        if self.when:\n",
    "            parts.append(f\"during {', '.join(self.when[:1])}\")\n",
    "        \n",
    "        # Why\n",
    "        if self.why:\n",
    "            parts.append(f\"because {self.why}\")\n",
    "        \n",
    "        # How\n",
    "        if self.how:\n",
    "            parts.append(f\"by {self.how}\")\n",
    "        \n",
    "        return ' '.join(parts)\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        \"\"\"Convert to dictionary for DataFrame.\"\"\"\n",
    "        return {\n",
    "            'who': ', '.join(self.who),\n",
    "            'what': self.what,\n",
    "            'when': ', '.join(self.when),\n",
    "            'where': ', '.join(self.where),\n",
    "            'why': self.why,\n",
    "            'how': self.how,\n",
    "            'sentence': self.to_sentence(),\n",
    "            'original_text': self.original_text[:200] + '...' if len(self.original_text) > 200 else self.original_text,\n",
    "            'chunk_index': self.chunk_index\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_w5h_events(text: str, chunk_index: int = 0) -> List[W5HEvent]:\n",
    "    \"\"\"Extract W5H events from a text chunk using spaCy.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    events = []\n",
    "    \n",
    "    # Process each sentence\n",
    "    for sent in doc.sents:\n",
    "        event = W5HEvent(original_text=sent.text.strip(), chunk_index=chunk_index)\n",
    "        \n",
    "        # Extract WHO (Person entities)\n",
    "        for ent in sent.ents:\n",
    "            if ent.label_ == \"PERSON\":\n",
    "                event.who.append(ent.text)\n",
    "            elif ent.label_ in [\"GPE\", \"LOC\", \"FAC\"]:\n",
    "                event.where.append(ent.text)\n",
    "            elif ent.label_ in [\"DATE\", \"TIME\"]:\n",
    "                event.when.append(ent.text)\n",
    "        \n",
    "        # Extract WHAT (main verb and object)\n",
    "        for token in sent:\n",
    "            if token.pos_ == \"VERB\" and token.dep_ == \"ROOT\":\n",
    "                # Get the verb and its direct objects\n",
    "                what_parts = [token.text]\n",
    "                for child in token.children:\n",
    "                    if child.dep_ in [\"dobj\", \"attr\", \"xcomp\"]:\n",
    "                        what_parts.append(child.text)\n",
    "                event.what = ' '.join(what_parts)\n",
    "                break\n",
    "        \n",
    "        # Extract WHY (causal indicators)\n",
    "        why_indicators = [\"because\", \"since\", \"as\", \"due to\", \"owing to\", \"for\"]\n",
    "        sent_lower = sent.text.lower()\n",
    "        for indicator in why_indicators:\n",
    "            if indicator in sent_lower:\n",
    "                idx = sent_lower.find(indicator)\n",
    "                if idx != -1:\n",
    "                    # Get the text after the indicator\n",
    "                    why_text = sent.text[idx + len(indicator):].strip()\n",
    "                    # Clean it up and take first clause\n",
    "                    why_text = why_text.split(',')[0].split('.')[0].strip()\n",
    "                    if len(why_text) > 5:  # Minimum meaningful length\n",
    "                        event.why = why_text[:100]  # Limit length\n",
    "                        break\n",
    "        \n",
    "        # Extract HOW (manner adverbs and phrases)\n",
    "        for token in sent:\n",
    "            if token.pos_ == \"ADV\" and token.dep_ in [\"advmod\", \"amod\"]:\n",
    "                # Check if it's a manner adverb (ends in -ly)\n",
    "                if token.text.endswith('ly'):\n",
    "                    event.how = token.text\n",
    "                    break\n",
    "            elif token.text.lower() in [\"by\", \"through\", \"with\", \"using\"]:\n",
    "                # Get the phrase after these prepositions\n",
    "                how_parts = []\n",
    "                for child in token.children:\n",
    "                    if child.dep_ == \"pobj\":\n",
    "                        how_parts.append(child.text)\n",
    "                if how_parts:\n",
    "                    event.how = ' '.join(how_parts)\n",
    "                    break\n",
    "        \n",
    "        # Only add events that have at least WHO or WHAT\n",
    "        if event.who or event.what:\n",
    "            events.append(event)\n",
    "    \n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract events from all chunks\n",
    "all_events = []\n",
    "\n",
    "print(\"Extracting W5H events from text chunks...\")\n",
    "for i, chunk in enumerate(tqdm(text_chunks)):\n",
    "    chunk_events = extract_w5h_events(chunk, chunk_index=i)\n",
    "    all_events.extend(chunk_events)\n",
    "\n",
    "print(f\"\\nExtracted {len(all_events)} events\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "events_df = pd.DataFrame([event.to_dict() for event in all_events])\n",
    "print(f\"\\nDataFrame shape: {events_df.shape}\")\n",
    "events_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze event extraction quality\n",
    "print(\"Event Extraction Statistics:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total events: {len(events_df)}\")\n",
    "print(f\"\\nField coverage:\")\n",
    "for col in ['who', 'what', 'when', 'where', 'why', 'how']:\n",
    "    non_empty = (events_df[col] != '').sum()\n",
    "    percentage = (non_empty / len(events_df)) * 100\n",
    "    print(f\"  {col.upper():6s}: {non_empty:5d} ({percentage:.1f}%)\")\n",
    "\n",
    "# Sample events with good coverage\n",
    "print(\"\\nSample events with multiple W5H fields:\")\n",
    "print(\"=\"*50)\n",
    "# Count non-empty fields for each event\n",
    "events_df['field_count'] = events_df[['who', 'what', 'when', 'where', 'why', 'how']].apply(\n",
    "    lambda x: (x != '').sum(), axis=1\n",
    ")\n",
    "rich_events = events_df[events_df['field_count'] >= 3].head(3)\n",
    "for idx, row in rich_events.iterrows():\n",
    "    print(f\"\\nEvent {idx}:\")\n",
    "    print(f\"Sentence: {row['sentence']}\")\n",
    "    print(f\"Original: {row['original_text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sentence transformer model\n",
    "print(\"Loading sentence transformer model...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(f\"Model loaded: {model}\")\n",
    "print(f\"Embedding dimension: {model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for event sentences\n",
    "print(\"Generating sentence embeddings...\")\n",
    "sentences = events_df['sentence'].tolist()\n",
    "\n",
    "# Generate embeddings in batches\n",
    "embeddings = model.encode(sentences, show_progress_bar=True, batch_size=32)\n",
    "\n",
    "print(f\"Generated {len(embeddings)} embeddings\")\n",
    "print(f\"Embedding shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. UMAP Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure UMAP\n",
    "print(\"Applying UMAP dimensionality reduction...\")\n",
    "\n",
    "# UMAP parameters\n",
    "reducer = umap.UMAP(\n",
    "    n_neighbors=30,\n",
    "    min_dist=0.1,\n",
    "    n_components=2,\n",
    "    metric='cosine',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "embeddings_2d = reducer.fit_transform(embeddings)\n",
    "\n",
    "print(f\"Reduced to shape: {embeddings_2d.shape}\")\n",
    "\n",
    "# Add UMAP coordinates to dataframe\n",
    "events_df['umap_x'] = embeddings_2d[:, 0]\n",
    "events_df['umap_y'] = embeddings_2d[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Add 3D UMAP for richer visualization\n",
    "print(\"Generating 3D UMAP...\")\n",
    "\n",
    "reducer_3d = umap.UMAP(\n",
    "    n_neighbors=30,\n",
    "    min_dist=0.1,\n",
    "    n_components=3,\n",
    "    metric='cosine',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "embeddings_3d = reducer_3d.fit_transform(embeddings)\n",
    "events_df['umap_x_3d'] = embeddings_3d[:, 0]\n",
    "events_df['umap_y_3d'] = embeddings_3d[:, 1]\n",
    "events_df['umap_z_3d'] = embeddings_3d[:, 2]\n",
    "\n",
    "print(\"3D UMAP complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster detection using DBSCAN\n",
    "print(\"Detecting clusters...\")\n",
    "\n",
    "clustering = DBSCAN(eps=0.5, min_samples=10)\n",
    "events_df['cluster'] = clustering.fit_predict(embeddings_2d)\n",
    "\n",
    "n_clusters = len(set(events_df['cluster'])) - (1 if -1 in events_df['cluster'] else 0)\n",
    "n_noise = list(events_df['cluster']).count(-1)\n",
    "\n",
    "print(f\"Found {n_clusters} clusters\")\n",
    "print(f\"Noise points: {n_noise}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interactive Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create color mapping for temporal progression\n",
    "events_df['color_temporal'] = events_df['chunk_index']\n",
    "\n",
    "# Create labels for hover text\n",
    "events_df['hover_text'] = events_df.apply(\n",
    "    lambda x: f\"<b>Who:</b> {x['who']}<br>\"\n",
    "              f\"<b>What:</b> {x['what']}<br>\"\n",
    "              f\"<b>Where:</b> {x['where']}<br>\"\n",
    "              f\"<b>When:</b> {x['when']}<br>\"\n",
    "              f\"<b>Why:</b> {x['why']}<br>\"\n",
    "              f\"<b>How:</b> {x['how']}<br>\"\n",
    "              f\"<b>Chunk:</b> {x['chunk_index']}<br>\"\n",
    "              f\"<b>Cluster:</b> {x['cluster']}\",\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D Interactive Visualization\n",
    "fig_2d = px.scatter(\n",
    "    events_df,\n",
    "    x='umap_x',\n",
    "    y='umap_y',\n",
    "    color='color_temporal',\n",
    "    hover_data=['sentence', 'original_text'],\n",
    "    custom_data=['hover_text'],\n",
    "    title=\"Benjamin Franklin's Autobiography: Event Manifold (2D UMAP)\",\n",
    "    labels={'color_temporal': 'Chronological Order', 'umap_x': 'UMAP 1', 'umap_y': 'UMAP 2'},\n",
    "    color_continuous_scale='Viridis',\n",
    "    width=1000,\n",
    "    height=700\n",
    ")\n",
    "\n",
    "# Update hover template\n",
    "fig_2d.update_traces(\n",
    "    hovertemplate='%{customdata[0]}<br><b>Sentence:</b> %{hoverdata[0]}<extra></extra>',\n",
    "    marker=dict(size=6, opacity=0.7)\n",
    ")\n",
    "\n",
    "fig_2d.update_layout(\n",
    "    hovermode='closest',\n",
    "    font=dict(size=12),\n",
    "    plot_bgcolor='white',\n",
    "    paper_bgcolor='white'\n",
    ")\n",
    "\n",
    "fig_2d.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D Interactive Visualization\n",
    "fig_3d = px.scatter_3d(\n",
    "    events_df,\n",
    "    x='umap_x_3d',\n",
    "    y='umap_y_3d',\n",
    "    z='umap_z_3d',\n",
    "    color='cluster',\n",
    "    hover_data=['sentence', 'who', 'what', 'where', 'when'],\n",
    "    title=\"Benjamin Franklin's Autobiography: Event Manifold (3D UMAP with Clusters)\",\n",
    "    labels={'cluster': 'Event Cluster'},\n",
    "    width=1000,\n",
    "    height=700,\n",
    "    color_discrete_sequence=px.colors.qualitative.Set3\n",
    ")\n",
    "\n",
    "fig_3d.update_traces(\n",
    "    marker=dict(size=4, opacity=0.8),\n",
    "    selector=dict(mode='markers')\n",
    ")\n",
    "\n",
    "fig_3d.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='UMAP 1',\n",
    "        yaxis_title='UMAP 2',\n",
    "        zaxis_title='UMAP 3',\n",
    "        bgcolor='white'\n",
    "    ),\n",
    "    font=dict(size=12)\n",
    ")\n",
    "\n",
    "fig_3d.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a density heatmap\n",
    "fig_density = px.density_heatmap(\n",
    "    events_df,\n",
    "    x='umap_x',\n",
    "    y='umap_y',\n",
    "    nbinsx=50,\n",
    "    nbinsy=50,\n",
    "    title=\"Event Density in Benjamin Franklin's Autobiography\",\n",
    "    labels={'umap_x': 'UMAP 1', 'umap_y': 'UMAP 2'},\n",
    "    color_continuous_scale='Blues',\n",
    "    width=900,\n",
    "    height=700\n",
    ")\n",
    "\n",
    "fig_density.update_layout(\n",
    "    font=dict(size=12),\n",
    "    plot_bgcolor='white',\n",
    "    paper_bgcolor='white'\n",
    ")\n",
    "\n",
    "fig_density.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analysis and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze clusters\n",
    "print(\"Cluster Analysis\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for cluster_id in sorted(events_df['cluster'].unique()):\n",
    "    if cluster_id == -1:\n",
    "        continue\n",
    "    \n",
    "    cluster_events = events_df[events_df['cluster'] == cluster_id]\n",
    "    print(f\"\\nCluster {cluster_id} ({len(cluster_events)} events):\")\n",
    "    \n",
    "    # Find most common entities and words\n",
    "    who_list = ' '.join(cluster_events['who'].values).split(', ')\n",
    "    who_list = [w for w in who_list if w]\n",
    "    \n",
    "    if who_list:\n",
    "        from collections import Counter\n",
    "        top_who = Counter(who_list).most_common(3)\n",
    "        print(f\"  Top people: {', '.join([f'{person} ({count})' for person, count in top_who])}\")\n",
    "    \n",
    "    where_list = ' '.join(cluster_events['where'].values).split(', ')\n",
    "    where_list = [w for w in where_list if w]\n",
    "    \n",
    "    if where_list:\n",
    "        top_where = Counter(where_list).most_common(3)\n",
    "        print(f\"  Top places: {', '.join([f'{place} ({count})' for place, count in top_where])}\")\n",
    "    \n",
    "    # Sample sentences\n",
    "    sample_sentences = cluster_events['sentence'].head(2).values\n",
    "    print(f\"  Sample events:\")\n",
    "    for sent in sample_sentences:\n",
    "        print(f\"    - {sent[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find key figures in the autobiography\n",
    "print(\"Key Figures in Benjamin Franklin's Autobiography\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "all_people = []\n",
    "for who_str in events_df['who']:\n",
    "    if who_str:\n",
    "        all_people.extend(who_str.split(', '))\n",
    "\n",
    "from collections import Counter\n",
    "people_counts = Counter(all_people)\n",
    "top_people = people_counts.most_common(15)\n",
    "\n",
    "for person, count in top_people:\n",
    "    print(f\"{person:20s}: {count:3d} mentions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a timeline visualization\n",
    "# Group events by chunk (temporal order) and show progression\n",
    "temporal_stats = events_df.groupby('chunk_index').agg({\n",
    "    'sentence': 'count',\n",
    "    'who': lambda x: len(' '.join(x).split(', ')),\n",
    "    'where': lambda x: len(' '.join(x).split(', '))\n",
    "}).reset_index()\n",
    "\n",
    "temporal_stats.columns = ['chunk_index', 'event_count', 'people_count', 'place_count']\n",
    "\n",
    "fig_timeline = make_subplots(\n",
    "    rows=3, cols=1,\n",
    "    subplot_titles=('Events per Chunk', 'People Mentioned', 'Places Mentioned'),\n",
    "    shared_xaxes=True\n",
    ")\n",
    "\n",
    "fig_timeline.add_trace(\n",
    "    go.Scatter(x=temporal_stats['chunk_index'], y=temporal_stats['event_count'],\n",
    "               mode='lines', name='Events', line=dict(color='blue')),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig_timeline.add_trace(\n",
    "    go.Scatter(x=temporal_stats['chunk_index'], y=temporal_stats['people_count'],\n",
    "               mode='lines', name='People', line=dict(color='green')),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig_timeline.add_trace(\n",
    "    go.Scatter(x=temporal_stats['chunk_index'], y=temporal_stats['place_count'],\n",
    "               mode='lines', name='Places', line=dict(color='red')),\n",
    "    row=3, col=1\n",
    ")\n",
    "\n",
    "fig_timeline.update_xaxes(title_text=\"Text Chunk (Chronological)\", row=3, col=1)\n",
    "fig_timeline.update_yaxes(title_text=\"Count\", row=1, col=1)\n",
    "fig_timeline.update_yaxes(title_text=\"Count\", row=2, col=1)\n",
    "fig_timeline.update_yaxes(title_text=\"Count\", row=3, col=1)\n",
    "\n",
    "fig_timeline.update_layout(\n",
    "    height=700,\n",
    "    title_text=\"Temporal Analysis of Benjamin Franklin's Autobiography\",\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig_timeline.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data for further analysis\n",
    "print(\"Saving processed data...\")\n",
    "\n",
    "# Save events with embeddings\n",
    "events_df.to_csv('franklin_events_w5h.csv', index=False)\n",
    "np.save('franklin_embeddings.npy', embeddings)\n",
    "\n",
    "print(\"Data saved:\")\n",
    "print(\"  - franklin_events_w5h.csv (events with W5H extraction and UMAP coordinates)\")\n",
    "print(\"  - franklin_embeddings.npy (original high-dimensional embeddings)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Conclusions\n",
    "\n",
    "This notebook successfully:\n",
    "\n",
    "1. **Downloaded and preprocessed** Benjamin Franklin's autobiography from Project Gutenberg\n",
    "2. **Extracted W5H events** (Who, What, When, Where, Why, How) using NLP techniques\n",
    "3. **Generated natural language sentences** from structured events\n",
    "4. **Created sentence embeddings** using transformer models\n",
    "5. **Applied UMAP** for dimensionality reduction\n",
    "6. **Visualized the event manifold** with interactive plots\n",
    "7. **Identified clusters** of related events and themes\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "- The UMAP visualization reveals natural groupings of events in Franklin's life\n",
    "- Temporal progression can be traced through the manifold\n",
    "- Clusters correspond to different themes, periods, or types of events\n",
    "- Key figures and locations emerge from the W5H analysis\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Fine-tune W5H extraction with more sophisticated NLP models\n",
    "- Experiment with different sentence transformer models\n",
    "- Add interactive filtering by W5H components\n",
    "- Compare with other autobiographies or historical texts\n",
    "- Implement path analysis to trace Franklin's life journey through the manifold"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}